<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Large Language Models Do Not Simulate Human Psychology | Yining's Blog</title><meta name=keywords content="LLM,Psychology"><meta name=description content="
Here are just some notes taken while I was reading.

Recently, some research has suggested that LLMs may even be able to simulate human psychology and can therefore replace human participants in psychological studies. We caution against this approach.

First, we provide conceptual arguments against the hypothesis that LLMs simulate human psychology.
We then present empiric evidence illustrating our arguments by demonstrating that slight changes to wording that correspond to large changes in meaning lead to notable discrepancies between LLMs&rsquo; and human responses, even for the recent CENTAUR model that was specifically fine-tuned on psychological responses.
Additionally, different LLMs show very different responses to novel items, further illustrating their lack of reliability.

We conclude that LLMs do not simulate human psychology and recommend that psychological researchers should treat LLMs as useful but fundamentally unreliable tools that need to be validated against human responses for every new application."><meta name=author content="Yining"><link rel=canonical href=https://yningg.github.io/Blogs/posts/research/llm_psychology/><meta name=google-site-verification content="G-R86690PBWG"><link crossorigin=anonymous href=/Blogs/assets/css/stylesheet.f49d66caae9ea0fd43f21f29e71a8d3e284517ed770f2aa86fa012953ad3c9ef.css integrity="sha256-9J1myq6eoP1D8h8p5xqNPihFF+13Dyqob6ASlTrTye8=" rel="preload stylesheet" as=style><link rel=icon href=https://yningg.github.io/Blogs/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://yningg.github.io/Blogs/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://yningg.github.io/Blogs/favicon-32x32.png><link rel=apple-touch-icon href=https://yningg.github.io/Blogs/apple-touch-icon.png><link rel=mask-icon href=https://yningg.github.io/Blogs/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://yningg.github.io/Blogs/posts/research/llm_psychology/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-R86690PBWG"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-R86690PBWG")}</script><meta property="og:url" content="https://yningg.github.io/Blogs/posts/research/llm_psychology/"><meta property="og:site_name" content="Yining's Blog"><meta property="og:title" content="Large Language Models Do Not Simulate Human Psychology"><meta property="og:description" content=" Here are just some notes taken while I was reading.
Recently, some research has suggested that LLMs may even be able to simulate human psychology and can therefore replace human participants in psychological studies. We caution against this approach.
First, we provide conceptual arguments against the hypothesis that LLMs simulate human psychology. We then present empiric evidence illustrating our arguments by demonstrating that slight changes to wording that correspond to large changes in meaning lead to notable discrepancies between LLMs’ and human responses, even for the recent CENTAUR model that was specifically fine-tuned on psychological responses. Additionally, different LLMs show very different responses to novel items, further illustrating their lack of reliability. We conclude that LLMs do not simulate human psychology and recommend that psychological researchers should treat LLMs as useful but fundamentally unreliable tools that need to be validated against human responses for every new application."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-08-18T00:00:00+00:00"><meta property="article:modified_time" content="2025-08-18T00:00:00+00:00"><meta property="article:tag" content="LLM"><meta property="article:tag" content="Psychology"><meta property="og:image" content="https://github.com/Yningg/Blogs/tree/master/static/apple-touch-icon.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://github.com/Yningg/Blogs/tree/master/static/apple-touch-icon.png"><meta name=twitter:title content="Large Language Models Do Not Simulate Human Psychology"><meta name=twitter:description content="
Here are just some notes taken while I was reading.

Recently, some research has suggested that LLMs may even be able to simulate human psychology and can therefore replace human participants in psychological studies. We caution against this approach.

First, we provide conceptual arguments against the hypothesis that LLMs simulate human psychology.
We then present empiric evidence illustrating our arguments by demonstrating that slight changes to wording that correspond to large changes in meaning lead to notable discrepancies between LLMs&rsquo; and human responses, even for the recent CENTAUR model that was specifically fine-tuned on psychological responses.
Additionally, different LLMs show very different responses to novel items, further illustrating their lack of reliability.

We conclude that LLMs do not simulate human psychology and recommend that psychological researchers should treat LLMs as useful but fundamentally unreliable tools that need to be validated against human responses for every new application."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://yningg.github.io/Blogs/posts/"},{"@type":"ListItem","position":2,"name":"Large Language Models Do Not Simulate Human Psychology","item":"https://yningg.github.io/Blogs/posts/research/llm_psychology/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Large Language Models Do Not Simulate Human Psychology","name":"Large Language Models Do Not Simulate Human Psychology","description":" Here are just some notes taken while I was reading.\nRecently, some research has suggested that LLMs may even be able to simulate human psychology and can therefore replace human participants in psychological studies. We caution against this approach.\nFirst, we provide conceptual arguments against the hypothesis that LLMs simulate human psychology. We then present empiric evidence illustrating our arguments by demonstrating that slight changes to wording that correspond to large changes in meaning lead to notable discrepancies between LLMs\u0026rsquo; and human responses, even for the recent CENTAUR model that was specifically fine-tuned on psychological responses. Additionally, different LLMs show very different responses to novel items, further illustrating their lack of reliability. We conclude that LLMs do not simulate human psychology and recommend that psychological researchers should treat LLMs as useful but fundamentally unreliable tools that need to be validated against human responses for every new application.\n","keywords":["LLM","Psychology"],"articleBody":" Here are just some notes taken while I was reading.\nRecently, some research has suggested that LLMs may even be able to simulate human psychology and can therefore replace human participants in psychological studies. We caution against this approach.\nFirst, we provide conceptual arguments against the hypothesis that LLMs simulate human psychology. We then present empiric evidence illustrating our arguments by demonstrating that slight changes to wording that correspond to large changes in meaning lead to notable discrepancies between LLMs’ and human responses, even for the recent CENTAUR model that was specifically fine-tuned on psychological responses. Additionally, different LLMs show very different responses to novel items, further illustrating their lack of reliability. We conclude that LLMs do not simulate human psychology and recommend that psychological researchers should treat LLMs as useful but fundamentally unreliable tools that need to be validated against human responses for every new application.\nIntroduction The core function of any LLM is to simply predict the probability of each possible next word (more precisely: the next token), randomly select the next word according to the predicted probabilities, and continue until all desired text is generated–with no explicit regard for meaning or truth.\nTask that particularly related to psychological science: simulating human participants’ responses and thus reducing or potentially eliminating the need for such participants.\nThe structure of a prompt\nA description of a person or a certain participant group the LLM is supposed to simulate (e.g., US American women over 40) A stimulus, such as a vignette The questionnaire the simulated participants are supposed to answer LLMs can, fundamentally, not simulate human psychology when dealing with novel scenarios that go too far beyond the LLMs training data.\nCritiques of LLMs as Simulators of Human Psychology Non-Human Reactions to Instructions: LLMs do not always react to instructions as intended\nZhu et al. [2024] empirically investigated LLMs as user simulations in recommender systems and found that recommendations only became accurate if the prompt contained a lot of information about the target user whereas less extensive prompting yielded inaccurate recommendations. Garcia et al. [2024] revealed that LLM moral judgments shifted more dramatically than human judgments depending on framing of moral scenarios. Wang et al. [2025] argue that the textual descriptions in typical prompts are too simplistic to accurately describe a human persona Inconsistency Across Simulations:\nAssigning human-like identities to LLMs does not lead to consistent human-like behavior Behaviors are highly sensitive to prompt formulations and model architectures Inability to Capture Human Diversity: LLMs are unable to reproduce the variance and diversity of human responses, even if prompted with different personas\nBiases of LLMs: biases tend to be different from human biases because training data is not representative of human diversity\n“Hallucinations”: Multiple authors report the tendency of LLMs to “hallucinate”, meaning the generation of factually incorrect or fictional content that appears superficially convincing\na simple explanation is that there is no mechanism inside an LLM that would distinguish between fact or fiction Theoretical Arguments:\n[Van Rooij et al., 2024] have provided a mathematical proof that it is computationally infeasible to find a computational model (such as an LLM) that responds like humans across all possible inputs, just based on observations\nThe training data containing examples of human-like responses to psychological stimuli does not imply that the trained LLM will also respond human-like to new stimuli – only that the text output will look superficially similar.\nDemonstrations of the Limitations of LLMs We test whether LLMs simulate human psychology when items are re-worded.\nHow can LLMs Support Psychological Research? We recommend that psychologists should refrain from using LLMs as participants for psychological studies.\nLLMs may be useful in other ways in psychological research, for example as tools for brainstorming, pilot testing, and refining experimental materials, perhaps even automating single, well-validated steps of data annotation. Crucially, however, researchers should remain able to validate all LLM outputs - and this is not possible if the LLMs produce the primary research data.\n","wordCount":"658","inLanguage":"en","image":"https://github.com/Yningg/Blogs/tree/master/static/apple-touch-icon.png","datePublished":"2025-08-18T00:00:00Z","dateModified":"2025-08-18T00:00:00Z","author":{"@type":"Person","name":"Yining"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://yningg.github.io/Blogs/posts/research/llm_psychology/"},"publisher":{"@type":"Organization","name":"Yining's Blog","logo":{"@type":"ImageObject","url":"https://yningg.github.io/Blogs/favicon.ico"}}}</script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]]},svg:{fontCache:"global"}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js></script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://yningg.github.io/Blogs/ accesskey=h title="Yining's Blog (Alt + H)">Yining's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://yningg.github.io/Blogs/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://yningg.github.io/Blogs/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://yningg.github.io/Blogs/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://yningg.github.io/Blogs/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://yningg.github.io/Blogs/>Home</a>&nbsp;»&nbsp;<a href=https://yningg.github.io/Blogs/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Large Language Models Do Not Simulate Human Psychology</h1><div class=post-meta><span title='2025-08-18 00:00:00 +0000 UTC'>August 18, 2025</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;658 words&nbsp;·&nbsp;Yining</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a></li><li><a href=#critiques-of-llms-as-simulators-of-human-psychology>Critiques of LLMs as Simulators of Human Psychology</a></li><li><a href=#demonstrations-of-the-limitations-of-llms>Demonstrations of the Limitations of LLMs</a></li><li><a href=#how-can-llms-support-psychological-research>How can LLMs Support Psychological Research?</a></li></ul></nav></div></details></div><div class=post-content><blockquote><p>Here are just some notes taken while I was reading.</p></blockquote><p>Recently, some research has suggested that LLMs may even be able to simulate human psychology and can therefore replace human participants in psychological studies. We caution against this approach.</p><ul><li>First, we provide <strong>conceptual arguments</strong> against the hypothesis that LLMs simulate human psychology.</li><li>We then present empiric evidence illustrating our arguments by demonstrating that <u>slight changes to wording that correspond to large changes in meaning lead to notable discrepancies between LLMs&rsquo; and human responses</u>, even for the recent CENTAUR model that was specifically fine-tuned on psychological responses.</li><li>Additionally, different LLMs show very different responses to novel items, further illustrating their <strong>lack of reliability</strong>.</li></ul><p>We conclude that <strong>LLMs do not simulate human psychology</strong> and recommend that psychological researchers should <strong>treat LLMs as useful but fundamentally unreliable tools</strong> that need to be validated against human responses for every new application.</p><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>The core function of any LLM is to simply predict the probability of each possible next word (more precisely: the next token), randomly select the next word according to the predicted probabilities, and continue until all desired text is generated&ndash;<u>with no explicit regard for meaning or truth</u>.</p><p><strong>Task</strong> that particularly related to psychological science: simulating human participants&rsquo; responses and thus reducing or potentially eliminating the need for such participants.</p><p><strong>The structure of a prompt</strong></p><ul><li>A description of a person or a certain participant group the LLM is supposed to simulate (e.g., US American women over 40)</li><li>A stimulus, such as a vignette</li><li>The questionnaire the simulated participants are supposed to answer</li></ul><blockquote><p>LLMs can, fundamentally, not simulate human psychology when dealing with novel scenarios that go too far beyond the LLMs training data.</p></blockquote><h2 id=critiques-of-llms-as-simulators-of-human-psychology>Critiques of LLMs as Simulators of Human Psychology<a hidden class=anchor aria-hidden=true href=#critiques-of-llms-as-simulators-of-human-psychology>#</a></h2><p><strong>Non-Human Reactions to Instructions</strong>: LLMs do not always react to instructions as intended</p><ul><li>Zhu et al. [2024] empirically investigated LLMs as user simulations in recommender systems and found that recommendations only became accurate if the prompt contained a lot of information about the target user whereas less extensive prompting yielded inaccurate recommendations.</li><li>Garcia et al. [2024] revealed that LLM moral judgments shifted more dramatically than human judgments depending on framing of moral scenarios.</li><li>Wang et al. [2025] argue that the textual descriptions in typical prompts are too simplistic to accurately describe a human persona</li></ul><p><strong>Inconsistency Across Simulations</strong>:</p><ul><li>Assigning human-like identities to LLMs does not lead to consistent human-like behavior</li><li>Behaviors are highly sensitive to prompt formulations and model architectures</li></ul><p><strong>Inability to Capture Human Diversity</strong>: LLMs are unable to reproduce the variance and diversity of human responses, even if prompted with different personas</p><p><strong>Biases of LLMs</strong>: biases tend to be different from human biases because training data is not representative of human diversity</p><p><strong>&ldquo;Hallucinations&rdquo;</strong>: Multiple authors report the tendency of LLMs to &ldquo;hallucinate&rdquo;, meaning the generation of factually incorrect or fictional content that appears superficially convincing</p><ul><li>a simple explanation is that there is no mechanism inside an LLM that would distinguish between fact or fiction</li></ul><p><strong>Theoretical Arguments</strong>:</p><ul><li><p>[Van Rooij et al., 2024] have provided a mathematical proof that it is computationally infeasible to find a computational model (such as an LLM) that responds like humans across all possible inputs, just based on observations</p></li><li><p>The training data containing examples of human-like responses to psychological stimuli does not imply that the trained LLM will also respond human-like to new stimuli – only that the text output will look superficially similar.</p></li></ul><h2 id=demonstrations-of-the-limitations-of-llms>Demonstrations of the Limitations of LLMs<a hidden class=anchor aria-hidden=true href=#demonstrations-of-the-limitations-of-llms>#</a></h2><p>We test whether LLMs simulate human psychology when items are re-worded.</p><h2 id=how-can-llms-support-psychological-research>How can LLMs Support Psychological Research?<a hidden class=anchor aria-hidden=true href=#how-can-llms-support-psychological-research>#</a></h2><ul><li><p>We recommend that psychologists should refrain from using LLMs as participants for psychological studies.</p></li><li><p>LLMs may be useful in other ways in psychological research, for example as tools for brainstorming, pilot testing, and refining experimental materials, perhaps even automating single, well-validated steps of data annotation. Crucially, however, <u>researchers should remain able to validate all LLM outputs - and this is not possible if the LLMs produce the primary research data</u>.</p></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://yningg.github.io/Blogs/tags/llm/>LLM</a></li><li><a href=https://yningg.github.io/Blogs/tags/psychology/>Psychology</a></li></ul><nav class=paginav><a class=next href=https://yningg.github.io/Blogs/posts/books/starstoy/><span class=title>Next »</span><br><span>Reading Notes: 《星星是冰冷的玩具》</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Large Language Models Do Not Simulate Human Psychology on x" href="https://x.com/intent/tweet/?text=Large%20Language%20Models%20Do%20Not%20Simulate%20Human%20Psychology&amp;url=https%3a%2f%2fyningg.github.io%2fBlogs%2fposts%2fresearch%2fllm_psychology%2f&amp;hashtags=LLM%2cPsychology"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Large Language Models Do Not Simulate Human Psychology on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fyningg.github.io%2fBlogs%2fposts%2fresearch%2fllm_psychology%2f&amp;title=Large%20Language%20Models%20Do%20Not%20Simulate%20Human%20Psychology&amp;summary=Large%20Language%20Models%20Do%20Not%20Simulate%20Human%20Psychology&amp;source=https%3a%2f%2fyningg.github.io%2fBlogs%2fposts%2fresearch%2fllm_psychology%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Large Language Models Do Not Simulate Human Psychology on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fyningg.github.io%2fBlogs%2fposts%2fresearch%2fllm_psychology%2f&title=Large%20Language%20Models%20Do%20Not%20Simulate%20Human%20Psychology"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Large Language Models Do Not Simulate Human Psychology on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fyningg.github.io%2fBlogs%2fposts%2fresearch%2fllm_psychology%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Large Language Models Do Not Simulate Human Psychology on whatsapp" href="https://api.whatsapp.com/send?text=Large%20Language%20Models%20Do%20Not%20Simulate%20Human%20Psychology%20-%20https%3a%2f%2fyningg.github.io%2fBlogs%2fposts%2fresearch%2fllm_psychology%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Large Language Models Do Not Simulate Human Psychology on telegram" href="https://telegram.me/share/url?text=Large%20Language%20Models%20Do%20Not%20Simulate%20Human%20Psychology&amp;url=https%3a%2f%2fyningg.github.io%2fBlogs%2fposts%2fresearch%2fllm_psychology%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Large Language Models Do Not Simulate Human Psychology on ycombinator" href="https://news.ycombinator.com/submitlink?t=Large%20Language%20Models%20Do%20Not%20Simulate%20Human%20Psychology&u=https%3a%2f%2fyningg.github.io%2fBlogs%2fposts%2fresearch%2fllm_psychology%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer><script src=https://giscus.app/client.js data-repo=nejckorasa/nejckorasa.github.io data-repo-id="MDEwOlJlcG9zaXRvcnkyNzQ0NDI2MTE=" data-category=Announcements data-category-id=DIC_kwDOEFupc84CpfL- data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=en crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2025 <a href=https://yningg.github.io/Blogs/>Yining's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>