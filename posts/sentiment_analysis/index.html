<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Recent Advances on Sentiment Analysis | Yining's Blog</title><meta name=keywords content="Sentiment Analysis,Natural Language Processing,Large Language Model"><meta name=description content="(Left appox. 3 papers to be updated in recent days)
Here is a literature note on selected recent advancements in sentiment analysis techniques. The papers, published between 2024 and 2025 to date, all involve the application and/or discussion of LLMs in sentiment analysis tasks.
Taxonomy of Sentiment Analysis Techniques
Here I combine the taxonomies adopted in [2] and [6] to involve all mentioned sentiment analysis techniques.

Lexicon-based Methods

Possible to measure gradations in sentiment
More intrinsically suited to types of questions social scientists often ask, for example:

Trends over time in average sentiment across large quantities of texts
Whether one group of texts is more negative than another




Traditional Machine Learning Models (e.g., Naive Bayes, Support Vector Machines (SVM))
Deep Learning Models (e.g., Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs))

Can learn hierarchical and sequential patterns in text, more suitable for sentiment analysis on informal social media platforms
Appealing for domain-specific applications
Performance degrades rapidly when methods are applied to other domains
Often require large labeled datasets and computational resources


Transformer-based Architectures (e.g., BERT, RoBERTa, and GPT)

Can address challenges specific to Twitter, including brevity, mixed sentiments, and multilingual content
Have high computational cost and require domain-specific adaptation
Have built-in bias even without domain-specific fine-tuning. Even explicitly de-biased LLMs continue to reflect pernicious biases.
Supervised/Fine-tuned LLMs

Pros: require fewer labeled samples
Cons:

May represent &ldquo;catastrophic forgetting&rdquo;, in which fine-tuning a general model for the sentiment analysis task risks losing many of the strengths of the pre-trained model
There are often unexpected (and unnoticed) pitfalls in trying to keep the training, validation, and application steps of the machine learning model separate, which may invalidate findings







Comparing to the supervised methods, unsupervised methods (i.e., Lexicon-based methods and unsupervised learning methods) are more domain-independent."><meta name=author content="Yining"><link rel=canonical href=https://yningg.github.io/Blogs/posts/sentiment_analysis/><meta name=google-site-verification content="G-R86690PBWG"><link crossorigin=anonymous href=/Blogs/assets/css/stylesheet.93f625d739f1d6a5c6f20c146bc6a8d26b233492b34b2220c54b12fd46a04ded.css integrity="sha256-k/Yl1znx1qXG8gwUa8ao0msjNJKzSyIgxUsS/UagTe0=" rel="preload stylesheet" as=style><link rel=icon href=https://yningg.github.io/Blogs/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://yningg.github.io/Blogs/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://yningg.github.io/Blogs/favicon-32x32.png><link rel=apple-touch-icon href=https://yningg.github.io/Blogs/apple-touch-icon.png><link rel=mask-icon href=https://yningg.github.io/Blogs/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://yningg.github.io/Blogs/posts/sentiment_analysis/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-R86690PBWG"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-R86690PBWG")}</script><meta property="og:url" content="https://yningg.github.io/Blogs/posts/sentiment_analysis/"><meta property="og:site_name" content="Yining's Blog"><meta property="og:title" content="Recent Advances on Sentiment Analysis"><meta property="og:description" content="(Left appox. 3 papers to be updated in recent days)
Here is a literature note on selected recent advancements in sentiment analysis techniques. The papers, published between 2024 and 2025 to date, all involve the application and/or discussion of LLMs in sentiment analysis tasks.
Taxonomy of Sentiment Analysis Techniques Here I combine the taxonomies adopted in [2] and [6] to involve all mentioned sentiment analysis techniques.
Lexicon-based Methods Possible to measure gradations in sentiment More intrinsically suited to types of questions social scientists often ask, for example: Trends over time in average sentiment across large quantities of texts Whether one group of texts is more negative than another Traditional Machine Learning Models (e.g., Naive Bayes, Support Vector Machines (SVM)) Deep Learning Models (e.g., Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs)) Can learn hierarchical and sequential patterns in text, more suitable for sentiment analysis on informal social media platforms Appealing for domain-specific applications Performance degrades rapidly when methods are applied to other domains Often require large labeled datasets and computational resources Transformer-based Architectures (e.g., BERT, RoBERTa, and GPT) Can address challenges specific to Twitter, including brevity, mixed sentiments, and multilingual content Have high computational cost and require domain-specific adaptation Have built-in bias even without domain-specific fine-tuning. Even explicitly de-biased LLMs continue to reflect pernicious biases. Supervised/Fine-tuned LLMs Pros: require fewer labeled samples Cons: May represent “catastrophic forgetting”, in which fine-tuning a general model for the sentiment analysis task risks losing many of the strengths of the pre-trained model There are often unexpected (and unnoticed) pitfalls in trying to keep the training, validation, and application steps of the machine learning model separate, which may invalidate findings Comparing to the supervised methods, unsupervised methods (i.e., Lexicon-based methods and unsupervised learning methods) are more domain-independent."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-06-26T00:00:00+00:00"><meta property="article:modified_time" content="2025-06-26T00:00:00+00:00"><meta property="article:tag" content="Sentiment Analysis"><meta property="article:tag" content="Natural Language Processing"><meta property="article:tag" content="Large Language Model"><meta property="og:image" content="https://github.com/Yningg/Blogs/tree/master/static/apple-touch-icon.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://github.com/Yningg/Blogs/tree/master/static/apple-touch-icon.png"><meta name=twitter:title content="Recent Advances on Sentiment Analysis"><meta name=twitter:description content="(Left appox. 3 papers to be updated in recent days)
Here is a literature note on selected recent advancements in sentiment analysis techniques. The papers, published between 2024 and 2025 to date, all involve the application and/or discussion of LLMs in sentiment analysis tasks.
Taxonomy of Sentiment Analysis Techniques
Here I combine the taxonomies adopted in [2] and [6] to involve all mentioned sentiment analysis techniques.

Lexicon-based Methods

Possible to measure gradations in sentiment
More intrinsically suited to types of questions social scientists often ask, for example:

Trends over time in average sentiment across large quantities of texts
Whether one group of texts is more negative than another




Traditional Machine Learning Models (e.g., Naive Bayes, Support Vector Machines (SVM))
Deep Learning Models (e.g., Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs))

Can learn hierarchical and sequential patterns in text, more suitable for sentiment analysis on informal social media platforms
Appealing for domain-specific applications
Performance degrades rapidly when methods are applied to other domains
Often require large labeled datasets and computational resources


Transformer-based Architectures (e.g., BERT, RoBERTa, and GPT)

Can address challenges specific to Twitter, including brevity, mixed sentiments, and multilingual content
Have high computational cost and require domain-specific adaptation
Have built-in bias even without domain-specific fine-tuning. Even explicitly de-biased LLMs continue to reflect pernicious biases.
Supervised/Fine-tuned LLMs

Pros: require fewer labeled samples
Cons:

May represent &ldquo;catastrophic forgetting&rdquo;, in which fine-tuning a general model for the sentiment analysis task risks losing many of the strengths of the pre-trained model
There are often unexpected (and unnoticed) pitfalls in trying to keep the training, validation, and application steps of the machine learning model separate, which may invalidate findings







Comparing to the supervised methods, unsupervised methods (i.e., Lexicon-based methods and unsupervised learning methods) are more domain-independent."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://yningg.github.io/Blogs/posts/"},{"@type":"ListItem","position":2,"name":"Recent Advances on Sentiment Analysis","item":"https://yningg.github.io/Blogs/posts/sentiment_analysis/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Recent Advances on Sentiment Analysis","name":"Recent Advances on Sentiment Analysis","description":"(Left appox. 3 papers to be updated in recent days)\nHere is a literature note on selected recent advancements in sentiment analysis techniques. The papers, published between 2024 and 2025 to date, all involve the application and/or discussion of LLMs in sentiment analysis tasks.\nTaxonomy of Sentiment Analysis Techniques Here I combine the taxonomies adopted in [2] and [6] to involve all mentioned sentiment analysis techniques.\nLexicon-based Methods Possible to measure gradations in sentiment More intrinsically suited to types of questions social scientists often ask, for example: Trends over time in average sentiment across large quantities of texts Whether one group of texts is more negative than another Traditional Machine Learning Models (e.g., Naive Bayes, Support Vector Machines (SVM)) Deep Learning Models (e.g., Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs)) Can learn hierarchical and sequential patterns in text, more suitable for sentiment analysis on informal social media platforms Appealing for domain-specific applications Performance degrades rapidly when methods are applied to other domains Often require large labeled datasets and computational resources Transformer-based Architectures (e.g., BERT, RoBERTa, and GPT) Can address challenges specific to Twitter, including brevity, mixed sentiments, and multilingual content Have high computational cost and require domain-specific adaptation Have built-in bias even without domain-specific fine-tuning. Even explicitly de-biased LLMs continue to reflect pernicious biases. Supervised/Fine-tuned LLMs Pros: require fewer labeled samples Cons: May represent \u0026ldquo;catastrophic forgetting\u0026rdquo;, in which fine-tuning a general model for the sentiment analysis task risks losing many of the strengths of the pre-trained model There are often unexpected (and unnoticed) pitfalls in trying to keep the training, validation, and application steps of the machine learning model separate, which may invalidate findings Comparing to the supervised methods, unsupervised methods (i.e., Lexicon-based methods and unsupervised learning methods) are more domain-independent.\n","keywords":["Sentiment Analysis","Natural Language Processing","Large Language Model"],"articleBody":"(Left appox. 3 papers to be updated in recent days)\nHere is a literature note on selected recent advancements in sentiment analysis techniques. The papers, published between 2024 and 2025 to date, all involve the application and/or discussion of LLMs in sentiment analysis tasks.\nTaxonomy of Sentiment Analysis Techniques Here I combine the taxonomies adopted in [2] and [6] to involve all mentioned sentiment analysis techniques.\nLexicon-based Methods Possible to measure gradations in sentiment More intrinsically suited to types of questions social scientists often ask, for example: Trends over time in average sentiment across large quantities of texts Whether one group of texts is more negative than another Traditional Machine Learning Models (e.g., Naive Bayes, Support Vector Machines (SVM)) Deep Learning Models (e.g., Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs)) Can learn hierarchical and sequential patterns in text, more suitable for sentiment analysis on informal social media platforms Appealing for domain-specific applications Performance degrades rapidly when methods are applied to other domains Often require large labeled datasets and computational resources Transformer-based Architectures (e.g., BERT, RoBERTa, and GPT) Can address challenges specific to Twitter, including brevity, mixed sentiments, and multilingual content Have high computational cost and require domain-specific adaptation Have built-in bias even without domain-specific fine-tuning. Even explicitly de-biased LLMs continue to reflect pernicious biases. Supervised/Fine-tuned LLMs Pros: require fewer labeled samples Cons: May represent “catastrophic forgetting”, in which fine-tuning a general model for the sentiment analysis task risks losing many of the strengths of the pre-trained model There are often unexpected (and unnoticed) pitfalls in trying to keep the training, validation, and application steps of the machine learning model separate, which may invalidate findings Comparing to the supervised methods, unsupervised methods (i.e., Lexicon-based methods and unsupervised learning methods) are more domain-independent.\nPaper 1: Unraveling media perspectives: a comprehensive methodology combining large language models, topic modeling, sentiment analysis, and ontology learning to analyse media bias Compare the performance of RoBERTa and spaCy\nRoBERTa: a powerful, pre-trained transformer-based language model that consistently achieves exceptional results across diverse NLP tasks, including sentiment analysis. spaCy: a versatile and widely used tool for natural language processing, and needs significantly less resources than transformers like RoBERTa Justification for minimal preprocessing dataset: minimal or no preprocessing yielded the best results for transformer models, depending on the specific dataset and transformer model used. In this context, minimal preprocessing refers to removing stopwords or converting text to lowercase.\nNonetheless, two optional preprocessing steps to increase the quality of results are recommended.\nExcluding non-English texts: this can increase the quality of results and ensure human understanding. Transformer models tend to perform best within the English language. Remove all text unrelated to the article’s topic: this can reduce the chance of topics being formed based on information that is not related to the actual topic of the article. Elements to be removed depend on the data source. E.g., advertisements, sections with related articles, and metadata about the article, like author and date in the text body. Paper 2: The advantages of lexicon-based sentiment analysis in an age of machine learning Many of automated methods share three important characteristics that limit their usefulness for general social science applications:\nInnovations in sentiment analysis have often been narrowly application- and domain-specific, which makes them unsuitable for comparisons across applications Most methods pay little or no attention to identifying a baseline or reference point. Many methods are unable to identify sentiment strength The reduction of sentiment to a binary classification–positive or negative only–risks drawing the wrong conclusions about patterns and trends in sentiment. (Another reason to adopt lexicon-based method) Being able to understand how a sentiment analysis method arrives at its assessment is often crucial in social science applications. However, many the machine learning approaches are black-box, or provide outputs such as feature weights that are almost impossible to interpret for human observers. Although it is possible to query LLMs for the “reasoning” behind a sentiment classification, their nature as “stochastic parrots” means that such reasoning cannot be relied upon.\nMultiLexScaled: a lexicon-based method that proposed in this paper\nWorking principle：average scores across eight widely used sentiment dictionaries Benefits: does not require any knowledge about the contents of the texts to be coded Paper 3: Weakly Supervised Deep Learning for Arabic Tweet Sentiment Analysis on Education Reforms: Leveraging Pre-Trained Models and LLMs With Snorkel Preprocessing of tweet dataset: removing non-essential elements, such as emojis, hashtags, @mentions, URLs, characters outside the Arabic alphabet, retweet indicators, duplicated tweets, letter repetitions, stop words, punctuation marks, numerical digits, and redundant spacing.\nPrompt Engineering: use LangChain framework to construct tailored instructions that guide the LLM in its sentiment classification task. Prompts included a task description,detailed instructions, and an expected output format.\nSystem = You are a helpful assistant working in the fields of Artificial Intelligence. You always respond in a professionally precise and accurate manner. Your answers and responds are always as short and as informative as possible. You are, also, honest and respond with (I don’t know.) in case you don’t have a clear answer. question = What is sentiment classification in NLP?\ninstructions = In the NLP task of sentiment classification, we try to label a piece of text as follows: 1. positive: if it has or contains a positive tone or lexicons.2. negative: if it has or contains a negative tone or lexicons.3. neutral: otherwise.You will be given a piece of text delimited by three hashs, label it based on the 3 classes above. ### {Text} ###\nOutput = “prediction”: string // The prediction of the included piece of text. Return only one of the following classes (positive, negative, neutral).\nPaper 4: GPT is an effective tool for multilingual psychological text analysis The social and behavioral sciences have been increasingly using automated text analysis to measure psychological constructs in text.\nGPT performed much better than English-language dictionary analysis at detecting psychological constructs. GPT performed nearly as well as, and sometimes better than, several top-performing fine-tuned machine learning models. Detailed results Sentiment: Even the oldest GPT model we analyzed, GPT-3.5Turbo, achieved good performance at predicting human ratings in both English Discrete Emotions: all versions of GPT had high agreement with humans in both English and Indonesian Offensiveness: We found high agreement between all versions of GPT and human ratings for English abd Turkish Sentiment and Discrete Emotions Measured on a Continuous Scale: GPT is capable of accurately detecting psychological constructs in text, regardless of the format of the ratings or the type of text GPT appears to be far more effective at detecting manually annotated sentiment and discrete emotions than common dictionary-based methods that are very popular in psychology and the social sciences Different versions of GPT provide very similar (albeit not exact) output for text analysis problems Analyses using GPT may potentially lead to very different conclusions than analyses using dictionary methods Moral Foundation: GPT may struggle more with more complex or difficult-to-define constructs Test–Retest Reliability of GPT:\nRunning GPT at separate times yields extremely high reliability when compared to traditional standards This suggests that GPT provides extremely reliable results even when the prompt is asked in a different language Paper 5: A Comparative Sentiment Analysis of Greek Clinical Conversations Using BERT, RoBERTa, GPT-2, and XLNet The categorization of utterances is as follows:\nMarked as positive when positive sentiments were expressed, such as satisfaction, relief, happiness, admiration, or gratitude. Marked as negative when negative sentiments are included, such as anger, pain, anxiety, etc. If an utterance lacks emotional expression or provides factual information, it is categorized as neutral sentiment. In the neutral category were also included general inquiries or educational information expressed by the clinicians to inform patients regarding their health condition. Paper 6: Sentiment Analysis of Twitter Data Using NLP Models: A Comprehensive Review Conducting sentiment analysis on Twitter data is notably complex for the following reasons:\nThe informal nature of the language used: tweets often contain slang, abbreviations, misspellings, emojis, hashtags, and context-dependent phrases, all of which add layers of complexity to the analysis. Twitter data is rife with phenomena such as sarcasm, irony, and ambiguous expressions that challenge even advanced NLP systems. Summary of the usage of GPT models Summary of the usage of BERT model Summary of the usage of RoBERTa model Overview of NLP models applied in sentiment analysis (Not only limited to Twitter datasets) Comparative analysis of model performance\nTransformer-based models, such as BERT, RoBERTa, and GPT variations, consistently exhibit superior performance BERT performs exceptionally well on tasks involving complex sentence structures RoBERTa often outperforms its predecessor, particularly in multi-class sentiment classification and domain-specific tasks, due to its enhanced training processes and larger training datasets GPT models, especially GPT-3 and GPT-3.5, show strong results in generating human-like text and handling nuanced context, although they sometimes require more data and fine-tuning for optimal sentiment analysis performance Traditional deep learning models remain effective for sequential data but often fall short compared to transformer-based architectures in understanding deeper contextual relationships The impact of pre-processing techniques Studies have shown that basic pre-processing methods help improve data quality and model interpretability.\nThe following techniques are applied to Twitter sentiment analysis.\nTokenization: critical for managing hashtags, mentions, and punctuation effectively. Can use methods like WordPiece or Byte Pair Encoding (BPE) Emoji and Hashtag Handling: convert emojis into textual equivalents (e.g., → happy) and split hashtags into component words (e.g., #HappyDay → Happy Day) Noise removal: includes eliminating retweets, URLs, mentions, and irrelevant symbols Stopword Removal: filters out common words (e.g., and, the) that do not carry significant sentiment information Normalization processes*: such as standardizing abbreviations, lowercase text, and handling elongated words (e.g., “cooool” “cool”), ensure uniformity in the dataset. Data Balancing: techniques such as oversampling, undersampling, or synthetic data generation are used to address class imbalance issues. In some cases, advanced techniques such as back-translation and synonym replacement are employed for data augmentation, enhancing model robustness.\nChallenges\nThe difficulty in detecting nuanced expressions such as sarcasm, irony, and mixed sentiments, which often require a level of language understanding that current models struggle to achieve. Although transformer-based models have advanced context understanding, they are not infallible when external knowledge or cultural context is necessary for accurate interpretation The limitation in cross-domain performance Ethical concerns, including biases in training data that can influence model output and reinforce harmful stereotypes References [1] Jähde, Orlando, Thorsten Weber, and Rüdiger Buchkremer. “Unraveling media perspectives: a comprehensive methodology combining large language models, topic modeling, sentiment analysis, and ontology learning to analyse media bias.” Journal of Computational Social Science 8, no. 2 (2025): 1-56.\n[2] van der Veen, A. Maurits, and Erik Bleich. “The advantages of lexicon-based sentiment analysis in an age of machine learning.” PloS one 20, no. 1 (2025): e0313092.\n[3] Alotaibi, Alanoud, Farrukh Nadeem, and Mohamed Hamdy. “Weakly Supervised Deep Learning for Arabic Tweet Sentiment Analysis on Education Reforms: Leveraging Pre-trained Models and LLMs with Snorkel.” IEEE Access (2025).\n[4] Rathje, Steve, Dan-Mircea Mirea, Ilia Sucholutsky, Raja Marjieh, Claire E. Robertson, and Jay J. Van Bavel. “GPT is an effective tool for multilingual psychological text analysis.” Proceedings of the National Academy of Sciences 121, no. 34 (2024): e2308950121.\n[5] Chatzimina, Maria Evangelia, Helen A. Papadaki, Charalampos Pontikoglou, and Manolis Tsiknakis. “A comparative sentiment analysis of Greek clinical conversations using BERT, RoBERTa, GPT-2, and XLNet.” Bioengineering 11, no. 6 (2024): 521.\n[6] Albladi, Aish, Minarul Islam, and Cheryl Seals. “Sentiment Analysis of Twitter data using NLP Models: A Comprehensive Review.” IEEE Access (2025).\n","wordCount":"1903","inLanguage":"en","image":"https://github.com/Yningg/Blogs/tree/master/static/apple-touch-icon.png","datePublished":"2025-06-26T00:00:00Z","dateModified":"2025-06-26T00:00:00Z","author":{"@type":"Person","name":"Yining"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://yningg.github.io/Blogs/posts/sentiment_analysis/"},"publisher":{"@type":"Organization","name":"Yining's Blog","logo":{"@type":"ImageObject","url":"https://yningg.github.io/Blogs/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://yningg.github.io/Blogs/ accesskey=h title="Yining's Blog (Alt + H)">Yining's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://yningg.github.io/Blogs/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://yningg.github.io/Blogs/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://yningg.github.io/Blogs/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://yningg.github.io/Blogs/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://yningg.github.io/Blogs/>Home</a>&nbsp;»&nbsp;<a href=https://yningg.github.io/Blogs/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Recent Advances on Sentiment Analysis</h1><div class=post-meta><span title='2025-06-26 00:00:00 +0000 UTC'>June 26, 2025</span>&nbsp;·&nbsp;9 min&nbsp;·&nbsp;1903 words&nbsp;·&nbsp;Yining</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#taxonomy-of-sentiment-analysis-techniques>Taxonomy of Sentiment Analysis Techniques</a><ul><li><a href=#paper-1-unraveling-media-perspectives-a-comprehensive-methodology-combining-large-language-models-topic-modeling-sentiment-analysis-and-ontology-learning-to-analyse-media-bias>Paper 1: Unraveling media perspectives: a comprehensive methodology combining large language models, topic modeling, sentiment analysis, and ontology learning to analyse media bias</a></li><li><a href=#paper-2-the-advantages-of-lexicon-based-sentiment--analysis-in-an-age-of-machine-learning>Paper 2: The advantages of lexicon-based sentiment analysis in an age of machine learning</a></li><li><a href=#paper-3-weakly-supervised-deep-learning-for-arabic-tweet-sentiment-analysis-on-education-reforms-leveraging-pre-trained-models-and-llms-with-snorkel>Paper 3: Weakly Supervised Deep Learning for Arabic Tweet Sentiment Analysis on Education Reforms: Leveraging Pre-Trained Models and LLMs With Snorkel</a></li><li><a href=#paper-4-gpt-is-an-effective-tool-for-multilingual-psychological-text-analysis>Paper 4: GPT is an effective tool for multilingual psychological text analysis</a></li><li><a href=#paper-5-a-comparative-sentiment-analysis-of-greek-clinical-conversations-using-bert-roberta-gpt-2-and-xlnet>Paper 5: A Comparative Sentiment Analysis of Greek Clinical Conversations Using BERT, RoBERTa, GPT-2, and XLNet</a></li><li><a href=#paper-6-sentiment-analysis-of-twitter-data-using-nlp-models-a-comprehensive-review>Paper 6: Sentiment Analysis of Twitter Data Using NLP Models: A Comprehensive Review</a></li></ul></li><li><a href=#references>References</a></li></ul></nav></div></details></div><div class=post-content><p>(Left appox. 3 papers to be updated in recent days)</p><p>Here is a literature note on selected recent advancements in sentiment analysis techniques. The papers, published between 2024 and 2025 to date, all involve the application and/or discussion of LLMs in sentiment analysis tasks.</p><h2 id=taxonomy-of-sentiment-analysis-techniques>Taxonomy of Sentiment Analysis Techniques<a hidden class=anchor aria-hidden=true href=#taxonomy-of-sentiment-analysis-techniques>#</a></h2><p>Here I combine the taxonomies adopted in [2] and [6] to involve all mentioned sentiment analysis techniques.</p><ol><li><strong>Lexicon-based Methods</strong><ul><li>Possible to measure gradations in sentiment</li><li>More intrinsically suited to types of questions social scientists often ask, for example:<ul><li>Trends over time in average sentiment across large quantities of texts</li><li>Whether one group of texts is more negative than another</li></ul></li></ul></li><li><strong>Traditional Machine Learning Models</strong> (e.g., Naive Bayes, Support Vector Machines (SVM))</li><li><strong>Deep Learning Models</strong> (e.g., Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs))<ul><li>Can learn hierarchical and sequential patterns in text, more suitable for sentiment analysis on informal social media platforms</li><li>Appealing for domain-specific applications</li><li>Performance degrades rapidly when methods are applied to other domains</li><li>Often require large labeled datasets and computational resources</li></ul></li><li><strong>Transformer-based Architectures</strong> (e.g., BERT, RoBERTa, and GPT)<ul><li>Can address challenges specific to Twitter, including brevity, mixed sentiments, and multilingual content</li><li>Have high computational cost and require domain-specific adaptation</li><li>Have built-in bias even without domain-specific fine-tuning. Even explicitly de-biased LLMs continue to reflect pernicious biases.</li><li>Supervised/Fine-tuned LLMs<ul><li>Pros: require fewer labeled samples</li><li>Cons:<ul><li>May represent &ldquo;catastrophic forgetting&rdquo;, in which fine-tuning a general model for the sentiment analysis task risks losing many of the strengths of the pre-trained model</li><li>There are often unexpected (and unnoticed) pitfalls in trying to keep the training, validation, and application steps of the machine learning model separate, which may invalidate findings</li></ul></li></ul></li></ul></li></ol><p>Comparing to the supervised methods, unsupervised methods (i.e., Lexicon-based methods and unsupervised learning methods) are more domain-independent.</p><h3 id=paper-1-unraveling-media-perspectives-a-comprehensive-methodology-combining-large-language-models-topic-modeling-sentiment-analysis-and-ontology-learning-to-analyse-media-bias>Paper 1: Unraveling media perspectives: a comprehensive methodology combining large language models, topic modeling, sentiment analysis, and ontology learning to analyse media bias<a hidden class=anchor aria-hidden=true href=#paper-1-unraveling-media-perspectives-a-comprehensive-methodology-combining-large-language-models-topic-modeling-sentiment-analysis-and-ontology-learning-to-analyse-media-bias>#</a></h3><blockquote><p>Compare the performance of RoBERTa and spaCy</p></blockquote><ul><li>RoBERTa: a powerful, pre-trained transformer-based language model that consistently achieves exceptional results across diverse NLP tasks, including sentiment analysis.</li><li>spaCy: a versatile and widely used tool for natural language processing, and needs significantly less resources than transformers like RoBERTa</li></ul><p><strong>Justification for minimal preprocessing dataset</strong>: minimal or no preprocessing yielded the best results for transformer models, depending on the specific dataset and transformer model used. In this context, minimal preprocessing refers to removing stopwords or converting text to lowercase.</p><p>Nonetheless, two optional preprocessing steps to increase the quality of results are recommended.</p><ul><li>Excluding non-English texts: this can increase the quality of results and ensure human understanding.<ul><li>Transformer models tend to perform best within the English language.</li></ul></li><li>Remove all text unrelated to the article’s topic: this can reduce the chance of topics being formed based on information that is not related to the actual topic of the article.<ul><li>Elements to be removed depend on the data source. E.g., advertisements, sections with related articles, and metadata about the article, like author and date in the text body.</li></ul></li></ul><h3 id=paper-2-the-advantages-of-lexicon-based-sentiment--analysis-in-an-age-of-machine-learning>Paper 2: The advantages of lexicon-based sentiment analysis in an age of machine learning<a hidden class=anchor aria-hidden=true href=#paper-2-the-advantages-of-lexicon-based-sentiment--analysis-in-an-age-of-machine-learning>#</a></h3><p>Many of automated methods share three important characteristics that limit their usefulness for general social science applications:</p><ol><li>Innovations in sentiment analysis have often been narrowly application- and domain-specific, which makes them unsuitable for comparisons across applications</li><li>Most methods pay little or no attention to identifying a baseline or reference point.</li><li>Many methods are unable to identify sentiment strength The reduction of sentiment to a binary classification–positive or negative only–risks drawing the wrong conclusions about patterns and trends in sentiment.</li></ol><p><em>(Another reason to adopt lexicon-based method)</em> Being able to understand how a sentiment analysis method arrives at its assessment is often crucial in social science applications. However, many the machine learning approaches are black-box, or provide outputs such as feature weights that are almost impossible to interpret for human observers. Although it is possible to query LLMs for the &ldquo;reasoning&rdquo; behind a sentiment classification, their nature as &ldquo;stochastic parrots&rdquo; means that such reasoning cannot be relied upon.</p><p><strong>MultiLexScaled</strong>: a lexicon-based method that proposed in this paper</p><ul><li>Working principle：average scores across eight widely used sentiment dictionaries</li><li>Benefits: does not require any knowledge about the contents of the texts to be coded</li></ul><h3 id=paper-3-weakly-supervised-deep-learning-for-arabic-tweet-sentiment-analysis-on-education-reforms-leveraging-pre-trained-models-and-llms-with-snorkel>Paper 3: Weakly Supervised Deep Learning for Arabic Tweet Sentiment Analysis on Education Reforms: Leveraging Pre-Trained Models and LLMs With Snorkel<a hidden class=anchor aria-hidden=true href=#paper-3-weakly-supervised-deep-learning-for-arabic-tweet-sentiment-analysis-on-education-reforms-leveraging-pre-trained-models-and-llms-with-snorkel>#</a></h3><p><strong>Preprocessing of tweet dataset</strong>: removing non-essential elements, such as emojis, hashtags, @mentions, URLs, characters outside the Arabic alphabet, retweet indicators, duplicated tweets, letter repetitions, stop words, punctuation marks, numerical digits, and redundant spacing.</p><p><strong>Prompt Engineering</strong>: use LangChain framework to construct tailored instructions that guide the LLM in its sentiment classification task. Prompts included a task description,detailed instructions, and an expected output format.</p><blockquote><p><strong>System</strong> = You are a helpful assistant working in the fields of Artificial Intelligence. You always respond in a professionally precise and accurate manner. Your answers and responds are always as short and as informative as possible. You are, also, honest and respond with (I don&rsquo;t know.) in case you don&rsquo;t have a clear answer.
<strong>question</strong> = What is sentiment classification in NLP?</p><p><strong>instructions</strong> = In the NLP task of sentiment classification, we try to label a piece of text as follows: 1. positive: if it has or contains a positive tone or lexicons.2. negative: if it has or contains a negative tone or lexicons.3. neutral: otherwise.You will be given a piece of text delimited by three hashs, label it based on the 3 classes above.
<strong>### {Text} ###</strong></p><p><strong>Output</strong> = &ldquo;prediction&rdquo;: string // The prediction of the
included piece of text. Return only one of the following
classes (positive, negative, neutral).</p></blockquote><h3 id=paper-4-gpt-is-an-effective-tool-for-multilingual-psychological-text-analysis>Paper 4: GPT is an effective tool for multilingual psychological text analysis<a hidden class=anchor aria-hidden=true href=#paper-4-gpt-is-an-effective-tool-for-multilingual-psychological-text-analysis>#</a></h3><p>The social and behavioral sciences have been increasingly using automated text analysis to measure psychological constructs in text.</p><ul><li>GPT performed much better than English-language dictionary analysis at detecting psychological constructs.</li><li>GPT performed nearly as well as, and sometimes better than, several top-performing fine-tuned machine learning models.</li></ul><h4 id=detailed-results>Detailed results<a hidden class=anchor aria-hidden=true href=#detailed-results>#</a></h4><ul><li><strong>Sentiment</strong>: Even the oldest GPT model we analyzed, GPT-3.5Turbo, achieved good performance at predicting human ratings in both English</li><li><strong>Discrete Emotions</strong>: all versions of GPT had high agreement with humans in both English and Indonesian</li><li><strong>Offensiveness</strong>: We found high agreement between all versions of GPT and human ratings for English abd Turkish</li><li><strong>Sentiment and Discrete Emotions Measured on a Continuous Scale</strong>:<ul><li>GPT is capable of accurately detecting psychological constructs in text, regardless of the format of the ratings or the type of text</li><li>GPT appears to be far more effective at detecting manually annotated sentiment and discrete emotions than common dictionary-based methods that are very popular in psychology and the social sciences</li><li>Different versions of GPT provide very similar (albeit not exact) output for text analysis problems</li><li>Analyses using GPT may potentially lead to very different conclusions than analyses using dictionary methods</li></ul></li><li><strong>Moral Foundation:</strong> GPT may struggle more with more complex or difficult-to-define constructs</li></ul><p><strong>Test–Retest Reliability of GPT</strong>:</p><ul><li>Running GPT at separate times yields extremely high reliability when compared to traditional standards</li><li>This suggests that GPT provides extremely reliable results even when the prompt is asked in a different language</li></ul><h3 id=paper-5-a-comparative-sentiment-analysis-of-greek-clinical-conversations-using-bert-roberta-gpt-2-and-xlnet>Paper 5: A Comparative Sentiment Analysis of Greek Clinical Conversations Using BERT, RoBERTa, GPT-2, and XLNet<a hidden class=anchor aria-hidden=true href=#paper-5-a-comparative-sentiment-analysis-of-greek-clinical-conversations-using-bert-roberta-gpt-2-and-xlnet>#</a></h3><p>The categorization of utterances is as follows:</p><ul><li>Marked as <strong>positive</strong> when positive sentiments were expressed, such as satisfaction, relief, happiness, admiration, or gratitude.</li><li>Marked as <strong>negative</strong> when negative sentiments are included, such as anger, pain, anxiety, etc.</li><li>If an utterance lacks emotional expression or provides factual information, it is categorized as <strong>neutral</strong> sentiment. In the neutral category were also included general inquiries or educational information expressed by the clinicians to inform patients regarding their health condition.</li></ul><h3 id=paper-6-sentiment-analysis-of-twitter-data-using-nlp-models-a-comprehensive-review>Paper 6: Sentiment Analysis of Twitter Data Using NLP Models: A Comprehensive Review<a hidden class=anchor aria-hidden=true href=#paper-6-sentiment-analysis-of-twitter-data-using-nlp-models-a-comprehensive-review>#</a></h3><p>Conducting sentiment analysis on Twitter data is notably complex for the following reasons:</p><ul><li>The informal nature of the language used: tweets often contain slang, abbreviations, misspellings, emojis, hashtags, and context-dependent phrases, all of which add layers of complexity to the analysis.</li><li>Twitter data is rife with phenomena such as sarcasm, irony, and ambiguous expressions that challenge even advanced NLP systems.</li></ul><p><strong>Summary of the usage of GPT models</strong>
<img alt="alt text" loading=lazy src=/Blogs/Figures/gpt_usage_summary.png></p><p><strong>Summary of the usage of BERT model</strong>
<img alt="alt text" loading=lazy src=/Blogs/Figures/bert_usage_summary.png></p><p><strong>Summary of the usage of RoBERTa model</strong>
<img alt="alt text" loading=lazy src=/Blogs/Figures/roberta_usage_summary.png></p><p><strong>Overview of NLP models applied in sentiment analysis</strong>
(Not only limited to Twitter datasets)
<img alt="alt text" loading=lazy src=/Blogs/Figures/meta_nlp_summary.png></p><p><strong>Comparative analysis of model performance</strong></p><ul><li>Transformer-based models, such as BERT, RoBERTa, and GPT variations, consistently exhibit superior performance<ul><li>BERT performs exceptionally well on tasks involving complex sentence structures</li><li>RoBERTa often outperforms its predecessor, particularly in multi-class sentiment classification and domain-specific tasks, due to its enhanced training processes and larger training datasets</li><li>GPT models, especially GPT-3 and GPT-3.5, show strong results in generating human-like text and handling nuanced context, although they sometimes require more data and fine-tuning for optimal sentiment analysis performance</li></ul></li><li>Traditional deep learning models remain effective for sequential data but often fall short compared to transformer-based architectures in understanding deeper contextual relationships</li></ul><p><strong>The impact of pre-processing techniques</strong>
Studies have shown that basic pre-processing methods help improve data quality and model interpretability.</p><p>The following techniques are applied to Twitter sentiment analysis.</p><ul><li>Tokenization: critical for managing hashtags, mentions, and punctuation effectively. Can use methods like WordPiece or Byte Pair Encoding (BPE)</li><li>Emoji and Hashtag Handling: convert emojis into textual equivalents (e.g., → happy) and split hashtags into component words (e.g., #HappyDay → Happy Day)</li><li>Noise removal: includes eliminating retweets, URLs, mentions, and irrelevant symbols</li><li>Stopword Removal: filters out common words (e.g., and, the) that do not carry significant sentiment information</li><li>Normalization processes*: such as standardizing abbreviations, lowercase text, and handling elongated words (e.g., &ldquo;cooool&rdquo; &ldquo;cool&rdquo;), ensure uniformity in the dataset.</li><li>Data Balancing: techniques such as oversampling, undersampling, or synthetic data generation are used to address class imbalance issues.</li></ul><p>In some cases, advanced techniques such as back-translation and synonym replacement are employed for data augmentation, enhancing model robustness.</p><p><strong>Challenges</strong></p><ul><li>The difficulty in detecting nuanced expressions such as sarcasm, irony, and mixed sentiments, which often require a level of language understanding that current models struggle to achieve. Although transformer-based models have advanced context understanding, they are not infallible when external knowledge or cultural context is necessary for accurate interpretation</li><li>The limitation in cross-domain performance</li><li>Ethical concerns, including biases in training data that can influence model output and reinforce harmful stereotypes</li></ul><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[1] Jähde, Orlando, Thorsten Weber, and Rüdiger Buchkremer. &ldquo;Unraveling media perspectives: a comprehensive methodology combining large language models, topic modeling, sentiment analysis, and ontology learning to analyse media bias.&rdquo; Journal of Computational Social Science 8, no. 2 (2025): 1-56.</p><p>[2] van der Veen, A. Maurits, and Erik Bleich. &ldquo;The advantages of lexicon-based sentiment analysis in an age of machine learning.&rdquo; PloS one 20, no. 1 (2025): e0313092.</p><p>[3] Alotaibi, Alanoud, Farrukh Nadeem, and Mohamed Hamdy. &ldquo;Weakly Supervised Deep Learning for Arabic Tweet Sentiment Analysis on Education Reforms: Leveraging Pre-trained Models and LLMs with Snorkel.&rdquo; IEEE Access (2025).</p><p>[4] Rathje, Steve, Dan-Mircea Mirea, Ilia Sucholutsky, Raja Marjieh, Claire E. Robertson, and Jay J. Van Bavel. &ldquo;GPT is an effective tool for multilingual psychological text analysis.&rdquo; Proceedings of the National Academy of Sciences 121, no. 34 (2024): e2308950121.</p><p>[5] Chatzimina, Maria Evangelia, Helen A. Papadaki, Charalampos Pontikoglou, and Manolis Tsiknakis. &ldquo;A comparative sentiment analysis of Greek clinical conversations using BERT, RoBERTa, GPT-2, and XLNet.&rdquo; Bioengineering 11, no. 6 (2024): 521.</p><p>[6] Albladi, Aish, Minarul Islam, and Cheryl Seals. &ldquo;Sentiment Analysis of Twitter data using NLP Models: A Comprehensive Review.&rdquo; IEEE Access (2025).</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://yningg.github.io/Blogs/tags/sentiment-analysis/>Sentiment Analysis</a></li><li><a href=https://yningg.github.io/Blogs/tags/natural-language-processing/>Natural Language Processing</a></li><li><a href=https://yningg.github.io/Blogs/tags/large-language-model/>Large Language Model</a></li></ul><nav class=paginav><a class=prev href=https://yningg.github.io/Blogs/posts/books/stars_toy/><span class=title>« Prev</span><br><span>Reading Notes: 《星星是冰冷的玩具》</span>
</a><a class=next href=https://yningg.github.io/Blogs/posts/books/puppymoney/><span class=title>Next »</span><br><span>Reading Notes: 《小狗钱钱》</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Recent Advances on Sentiment Analysis on x" href="https://x.com/intent/tweet/?text=Recent%20Advances%20on%20Sentiment%20Analysis&amp;url=https%3a%2f%2fyningg.github.io%2fBlogs%2fposts%2fsentiment_analysis%2f&amp;hashtags=SentimentAnalysis%2cNaturalLanguageProcessing%2cLargeLanguageModel"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Recent Advances on Sentiment Analysis on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fyningg.github.io%2fBlogs%2fposts%2fsentiment_analysis%2f&amp;title=Recent%20Advances%20on%20Sentiment%20Analysis&amp;summary=Recent%20Advances%20on%20Sentiment%20Analysis&amp;source=https%3a%2f%2fyningg.github.io%2fBlogs%2fposts%2fsentiment_analysis%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Recent Advances on Sentiment Analysis on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fyningg.github.io%2fBlogs%2fposts%2fsentiment_analysis%2f&title=Recent%20Advances%20on%20Sentiment%20Analysis"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Recent Advances on Sentiment Analysis on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fyningg.github.io%2fBlogs%2fposts%2fsentiment_analysis%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Recent Advances on Sentiment Analysis on whatsapp" href="https://api.whatsapp.com/send?text=Recent%20Advances%20on%20Sentiment%20Analysis%20-%20https%3a%2f%2fyningg.github.io%2fBlogs%2fposts%2fsentiment_analysis%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Recent Advances on Sentiment Analysis on telegram" href="https://telegram.me/share/url?text=Recent%20Advances%20on%20Sentiment%20Analysis&amp;url=https%3a%2f%2fyningg.github.io%2fBlogs%2fposts%2fsentiment_analysis%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Recent Advances on Sentiment Analysis on ycombinator" href="https://news.ycombinator.com/submitlink?t=Recent%20Advances%20on%20Sentiment%20Analysis&u=https%3a%2f%2fyningg.github.io%2fBlogs%2fposts%2fsentiment_analysis%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer><script src=https://giscus.app/client.js data-repo=nejckorasa/nejckorasa.github.io data-repo-id="MDEwOlJlcG9zaXRvcnkyNzQ0NDI2MTE=" data-category=Announcements data-category-id=DIC_kwDOEFupc84CpfL- data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=en crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2025 <a href=https://yningg.github.io/Blogs/>Yining's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>